{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d69af8c-a8aa-4a15-a49e-c8eb78cee012",
   "metadata": {},
   "source": [
    "<center><img src='../../img/ai4eo_logos.jpg' alt='Logos AI4EO MOOC' width='80%'></img></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fd72bf-1892-48ff-847f-ced997848720",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b97f920-539c-4001-93ec-52f622939c50",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "897aa3d6-3d1f-4cbf-a3e1-0e068152a940",
   "metadata": {},
   "source": [
    "<a href=\"../00_index.ipynb\"><< Index</a><br>\n",
    "<a href=\"./\"><< Placeholder 1</a><span style=\"float:right;\"><a href=\"./\">Placeholder 2 >></a></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a02f2a-9255-466b-9732-894b633696ae",
   "metadata": {},
   "source": [
    "# 3H - Aesthetics Aware Reinforcement Learning for Image-Cropping (Training)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6090e87e-1371-44fc-a2f1-cedea037c478",
   "metadata": {},
   "source": [
    "<i>by Carlos Fortuny-Lombraña, EUMETSAT, Darmstadt, Germany</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee0217e-f24f-4590-bf15-48f9efee69ae",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22412fc-3819-4740-ba62-612f5b90e382",
   "metadata": {},
   "source": [
    "## Watch the video tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f497bc24-12a6-472a-a861-eb54a660005e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div align=\"center\"><iframe src=\"https://player.vimeo.com/video/510225048\" width=\"640\" height=\"360\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture\" allowfullscreen align=\"middle\"></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<div align=\"center\"><iframe src=\"https://player.vimeo.com/video/510225048\" width=\"640\" height=\"360\" frameborder=\"0\" allow=\"autoplay; fullscreen; picture-in-picture\" allowfullscreen align=\"middle\"></iframe></div>')     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15ec0b0-6e4c-40e8-b615-a7f8f2d3e68f",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91eadbc9-3148-4728-9c96-fbf9bd017db2",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51b64c01-7558-46d5-ad90-15299aee7b5d",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "*Why are we doing this?*\n",
    "\n",
    "Image cropping is a popular image editing operation that tries to remove well-composed areas from poorly composed images. It has the potential to increase picture visual quality since composition is a key factor in image quality. An effective automated picture cropping algorithm may provide editors with expert guidance while also saving them a significant amount of time [[1]](#source-1). Then, image cutting is an operation that can be linked with object detection and image segmentation.\n",
    "\n",
    "**Reinforcement learning**-based strategies have been successfully applied in many domains of computer vision, and thus, it is an interesting solution to be applied in many EO use case studies. This machine learning technique is based on learning from the mistakes (trial and error method), similar to the humans do. This is because AI agents should take actions in an environment to maximize the total reward. This type of learning is completely different to the ones shown previously, and one of the EO applications of reinforcement learning can be automatic image cropping using satellite imagery.\n",
    "\n",
    "<hr>\n",
    "\n",
    "*What will this workflow show you?* \n",
    "\n",
    "This notebook is showing the concept of reinforcement learning applied to EO applications. Then, this workflow guides you through auto-image cutting using [Sentinel-2 imagery data]((https://www.wekeo.eu/data?view=dataset&dataset=EO%3AESA%3ADAT%3ASENTINEL-2%3AMSI). It will lead you through the [A2-RL model architecture](https://github.com/wuhuikai/TF-A2RL) utilized in order to the image cropping process. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab331fc-eda4-4ae8-98f9-406359782d56",
   "metadata": {},
   "source": [
    "## Machine-Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f06a76-56b3-4e2b-83d5-4fb83df14b27",
   "metadata": {},
   "source": [
    "In this workflow, you will learn how a reinforcement learning-based method for automatic image cropping can be applied to satellite imagery.\n",
    "\n",
    "Here, an **A2-RL model**, which is based on the VFN (View Finding Network) model, developed by the *National Laboratory of Pattern Recognition* is utilized. The detailed architecture of the framework is explained later on. The A2-RL  model contains a reinforcement learning agent built for executing the sampled action to shrink the cropping window. Also, the reinforcement learning architecture is composed of **fully connected layers** and an **LSTM** (Long Short-Term Memory) **layer** (for memorizing the historical observations). The algorithm used for image cropping policy is the **asynchronous advantage actor-critic** (A3C). In addition, a **CNN** (Convolutional Neural Network), which is part of supervised learning, will be used for a local feature in the image [[1]](#source-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d8c69b-c4b5-402e-8337-52dc240c9e12",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4076e1b0-14da-41ef-b636-495953cc4eba",
   "metadata": {},
   "source": [
    "This workflow uses the following data:\n",
    "* A pre-trained model [`vfn_rl.pkl`](./vfn_rl.pkl) is utilized. \n",
    "* Real satellite images can be retrieved from the [WEkEO Platform](https://www.wekeo.eu/data?view=viewer). With the viewer interface, one can add the\n",
    "[Sentinel-2 MSI: True color layer](https://www.wekeo.eu/data?view=dataset&dataset=EO%3AESA%3ADAT%3ASENTINEL-2%3AMSI) (`dataset_ID` = \n",
    "EO:ESA:DAT:SENTINEL-2:MSI). The Sentinel-2 provides high-resolution optical imagery (Level-1C/Level-2C) which can be applied in the automatic cropping process. Then, the images are collected with the aid of the snipping tool (**Note**: Uncheck the *coastline & reference features* layer in the Settings). The alternative approach is collecting the data from the API. The two images used are:\n",
    "    1. Location: [Atokos](https://www.wekeo.eu/data?view=viewer&t=1532692856692&z=0&center=20.81023%2C38.48115&zoom=20.72&layers=W3siaWQiOiJjMCIsImxheWVySWQiOiJFTzpFU0E6REFUOlNFTlRJTkVMLTI6TVNJL19fREVGQVVMVF9fLzFfVFJVRV9DT0xPUiIsInpJbmRleCI6MTB9XQ%3D%3D), Greece - Island. <br />\n",
    "       Date: 27$^{\\text{th}}$ of July 2018.\n",
    "    2. Location:  [Vesuvius](https://www.wekeo.eu/data?view=viewer&t=1562111376377&z=0&center=14.39623%2C40.81018&zoom=20.94&layers=W3siaWQiOiJjMCIsImxheWVySWQiOiJFTzpFU0E6REFUOlNFTlRJTkVMLTI6TVNJL19fREVGQVVMVF9fLzFfVFJVRV9DT0xPUiIsInpJbmRleCI6MTB9XQ%3D%3D&initial=1), Italy - Volcano.  <br />\n",
    "       Date : 2$^{\\text{nd}}$ of July 2019.\n",
    "\n",
    "The workflow in this notebook does not contain the process of training. This is because training a deep neural network is computationally expensive, and the use of GPU is needed to speed up. Hence, the running time is being reduced by using pre-trained running. This notebook wants to show how **reinforcement learning** can be applied to Earth observation imagery.\n",
    "\n",
    "The script shown in this notebook is inspired by: [A2-RL: Aesthetics Aware Reinforcement Learning for Image Cropping](https://github.com/wuhuikai/TF-A2RL) repository [[1]](#source-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2b9d5a-c610-48bf-9012-5aa46028e6df",
   "metadata": {},
   "source": [
    "## Further Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ab13f3-365c-40af-bb69-c75843e25713",
   "metadata": {},
   "source": [
    "A comprehensive of relevant resources to expand your knowledge in reinforcement learning applied to EO applications is listed below:\n",
    "\n",
    "__Courses:__\n",
    "* [MIT 6.S191 - Introduction to Deep Learning](http://introtodeeplearning.com/)\n",
    "\n",
    "__Related Work:__ <a id='related_work'></a>\n",
    "* *Scientific Articles:*\n",
    "    * **[1]** <a id='source-1'></a> Li, D., Wu, H., Zhang, J., & Huang, K. (2018). A2-RL: Aesthetics aware reinforcement learning for image cropping. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 8193-8201). [[**paper**](https://arxiv.org/abs/1709.04595)] [[**GitHub repository**](https://github.com/wuhuikai/TF-A2RL)]\n",
    "    * **[2]** <a id='source-2'></a> Bueno, M. B., Nieto, X. G. I., Marqués, F., & Torres, J. (2017). Hierarchical object detection with deep reinforcement learning. Deep Learning for Image Processing Applications. [[**paper**](https://arxiv.org/pdf/1611.03718.pdf)] [[**GitHub repository**](https://imatge-upc.github.io/detection-2016-nipsws/)]\n",
    "    * **[3]** <a id='source-3'></a> Uzkent, B., & Ermon, S. (2020). Learning when and where to zoom with deep reinforcement learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 12345-12354). [[**paper**](https://openaccess.thecvf.com/content_CVPR_2020/papers/Uzkent_Learning_When_and_Where_to_Zoom_With_Deep_Reinforcement_Learning_CVPR_2020_paper.pdf)] [[**GitHub repository**](https://github.com/ermongroup/PatchDrop)]\n",
    "    * **[4]** <a id='source-4'></a> Uzkent, B., Yeh, C., & Ermon, S. (2020). Efficient object detection in large images using deep reinforcement learning. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (pp. 1824-1833). [[**paper**](http://openaccess.thecvf.com/content_WACV_2020/papers/Uzkent_Efficient_Object_Detection_in_Large_Images_Using_Deep_Reinforcement_Learning_WACV_2020_paper.pdf)] [[**GitHub repository**](https://github.com/uzkent/EfficientObjectDetection)]\n",
    "    * **[5]** <a id='source-5'></a> Mou, L., Saha, S., Hua, Y., Bovolo, F., Bruzzone, L., & Zhu, X. X. (2021). Deep reinforcement learning for band selection in hyperspectral image classification. IEEE Transactions on Geoscience and Remote Sensing.[[**paper**](https://ieeexplore.ieee.org/iel7/36/4358825/09387453.pdf)]\n",
    "    * **[6]** <a id='source-6'></a> Huang, K., Nie, W., & Luo, N. (2019). Fully polarized SAR imagery classification based on deep reinforcement learning method using multiple polarimetric features. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 12(10), 3719-3730. [[**paper**](https://ieeexplore.ieee.org/abstract/document/8713855/)]\n",
    "    * **[7]** <a id='source-7'></a> Zhao, D., Ma, Y., Jiang, Z., & Shi, Z. (2017). Multiresolution airport detection via hierarchical reinforcement learning saliency model. IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing, 10(6), 2855-2866. [[**paper**](https://ieeeprojects.eminents.in/uploads/basepaper/ETSIP016-2017.pdf)]\n",
    "    * **[8]** <a id='source-8'></a> Feng, J., Li, D., Gu, J., Cao, X., Shang, R., Zhang, X., & Jiao, L. (2021). Deep Reinforcement Learning for Semisupervised Hyperspectral Band Selection. IEEE Transactions on Geoscience and Remote Sensing. [[**paper**](https://ieeexplore.ieee.org/abstract/document/9358199/)]\n",
    "    * **[9]** <a id='source-9'></a> Li, X., Zheng, H., Han, C., Wang, H., Dong, K., Jing, Y., & Zheng, W. (2020). Cloud Detection of SuperView-1 Remote Sensing Images Based on Genetic Reinforcement Learning. Remote Sensing, 12(19), 3190. [[**paper**](https://www.mdpi.com/2072-4292/12/19/3190/pdf)]\n",
    "    * **[10]** <a id='source-10'></a> Ayush, K., Uzkent, B., Lobell, K. T. M. B. D., & Ermon, S. (2021, May). Efficient Poverty Mapping from High Resolution Remote Sensing Images. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 35, No. 1, pp. 12-20). [[**paper**](https://www.aaai.org/AAAI21Papers/AAAI-10300.AyushK.pdf)]\n",
    "    * **[11]** <a id='source-11'></a> Wang, W., Shen, J., & Ling, H. (2018). A deep network solution for attention and aesthetics aware photo cropping. IEEE transactions on pattern analysis and machine intelligence, 41(7), 1531-1544. [[**paper**](https://ieeexplore.ieee.org/abstract/document/8365844/)]\n",
    "    * **[12]** <a id='source-12'></a> Zhang, L., Xia, G. S., Wu, T., Lin, L., & Tai, X. C. (2016). Deep learning for remote sensing image understanding. [[**paper**](https://downloads.hindawi.com/journals/js/2016/7954154.pdf)]\n",
    "* *Books:*\n",
    "    * **[13]** <a id='source-13'></a> Sutton, R. S., & Barto, A. G. (1998). Introduction to reinforcement learning (Vol. 135). Cambridge: MIT press. [[**PDF**](https://login.cs.utexas.edu/sites/default/files/legacy_files/research/documents/1%20intro%20up%20to%20RL%3ATD.pdf)]\n",
    "* *Presentation slides:*\n",
    "    * Seminar on Machine Learning for Remote Sensing Group at the University of Maryland [[**Presentation slides**](https://uzkent.github.io/files/Qualcomm.pdf)]\n",
    "    \n",
    "\n",
    "__Documentation:__\n",
    "* [*Agent* Library for Reinforcement Learning in TensorFlow](https://www.tensorflow.org/agents)\n",
    "* [Reinforcement Learning Algorithms and Environments](https://gym.openai.com/docs/)\n",
    "\n",
    "__Data:__\n",
    "* [EuroSat Dataset](https://www.kaggle.com/apollo2506/eurosat-dataset)\n",
    "* [Airbus Ship Detection Challenge](https://www.kaggle.com/c/airbus-ship-detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b986b6-ab13-4bb9-80ba-a194964ee7cb",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80c51a1-296a-4c92-912f-7d12f627814a",
   "metadata": {},
   "source": [
    "## Notebook outline\n",
    "* [1 - Introduction to Reinforcement Learning and its applications in EO](#application_EO)\n",
    "* [2 - Architecture used for automatic image cropping](#image_cropping)\n",
    "* [3 - Examples using Sentinel-2 images](#sentinel)\n",
    "* [4 - Conclusions](#conclusions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb247fb-6d87-411c-8d78-a5de5fc1199d",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d443760b-9a72-413d-a42d-59384cb9a5a4",
   "metadata": {},
   "source": [
    "#### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c858fb54-4fa4-48dd-a7f9-d6b19ad06311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove warning messages\n",
    "import warnings  # a library to manage warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# Python packages\n",
    "from __future__ import absolute_import # Switch import's behaviour to absolute imports\n",
    "import os # a library that gives us access to various command line tools to support pathing\n",
    "import random # a library that implements pseudo-random number generators for various distributions\n",
    "import pickle # a library that implements binary protocols for serializing and de-serializing a Python object structure (mainly for dictionaries)\n",
    "import argparse # a recommended command-line parsing Python module\n",
    "import numpy as np # Python's array manipulation library \n",
    "import tensorflow.compat.v1 as tf #Importing the TensorFlow Version 1 for ML\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)  #Remove all the error messages due to TensorFlow\n",
    "tf.disable_v2_behavior() # Disable Tensorflow Version 2\n",
    "import skimage.io as io # a library that cointains a collection of algorithms for image processing\n",
    "from skimage import img_as_ubyte #  a function utilized for converting images to the desired dtype and properly rescale their values\n",
    "import easydict # a library used to access dictionary values as attributes\n",
    "import matplotlib.pyplot as plt  # the basic Python plotting library figure tools\n",
    "import ipyplot # a rich toolkit to help you make the most out of using Python interactively\n",
    "from IPython.display import Image, display # public API for display tools in IPython"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb3ff922-423f-42cf-89ad-6c389fd7ad78",
   "metadata": {},
   "source": [
    "#### Selection of a random seed \n",
    "\n",
    "The agent's action can be performed in different orders. Thus, each time the notebook is executed with a constant seed to reproduce the results. \n",
    "\n",
    "**Optional**: Play around with the seed number in order to see if the actions taken from the agent are still the same. The default seed used is *3*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6293829a-9ec8-41bd-b841-41056892de8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this constant seed\n",
    "SEED = 3 \n",
    "\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "# `python` built-in pseudo-random generator\n",
    "random.seed(SEED) \n",
    "# numpy pseudo-random generator\n",
    "np.random.seed(SEED) \n",
    "# tensorflow pseudo-random generator\n",
    "tf.set_random_seed(SEED)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3a3fd2-481c-4f00-bbe2-4944c332660c",
   "metadata": {},
   "source": [
    "#### Load helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ce4de61-a705-4111-bc25-33f67ae041be",
   "metadata": {},
   "outputs": [
    {
     "ename": "Exception",
     "evalue": "File `'./A2RL-functions.ipynb.py'` not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\envs\\mooc_ai4eo\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[0;32m    702\u001b[0m             \u001b[0mfpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marg_lst\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 703\u001b[1;33m             \u001b[0mfilename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfile_finder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    704\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\envs\\mooc_ai4eo\\lib\\site-packages\\IPython\\utils\\path.py\u001b[0m in \u001b[0;36mget_py_filename\u001b[1;34m(name, force_win32)\u001b[0m\n\u001b[0;32m    108\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'File `%r` not found.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: File `'./A2RL-functions.ipynb.py'` not found.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mException\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-90bb1afa0f9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Loading function Python script in the same directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'run'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'./A2RL-functions.ipynb'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\envs\\mooc_ai4eo\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[1;34m(self, magic_name, line, _stack_depth)\u001b[0m\n\u001b[0;32m   2346\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'local_ns'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_local_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2347\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2348\u001b[1;33m                 \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2349\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, parameter_s, runner, file_finder)\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\envs\\mooc_ai4eo\\lib\\site-packages\\IPython\\core\\magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[1;34m(f, *a, **k)\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[1;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 187\u001b[1;33m         \u001b[0mcall\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    189\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\Program Files\\Anaconda3\\envs\\mooc_ai4eo\\lib\\site-packages\\IPython\\core\\magics\\execution.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, parameter_s, runner, file_finder)\u001b[0m\n\u001b[0;32m    712\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'nt'\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mr\"^'.*'$\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    713\u001b[0m                 \u001b[0mwarn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'For Windows, use double quotes to wrap a filename: %run \"mypath\\\\myfile.py\"'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 714\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    715\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    716\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mfpath\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeta_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mException\u001b[0m: File `'./A2RL-functions.ipynb.py'` not found."
     ]
    }
   ],
   "source": [
    "# Loading function Python script in the same directory\n",
    "%run ./A2RL-functions.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a66403df-ed09-469c-84c5-a3b99dac3f6d",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0223d7bc-000a-413d-a878-ebce885154d0",
   "metadata": {},
   "source": [
    "## 1 - Introduction to Reinforcement Learning and its applications in EO <a id='application_EO'></a>\n",
    "\n",
    "First of all, **reinforcement learning** is a machine learning approach that allows an agent to learn in an interactive environment through trial and error based on feedback from its own actions and experiences [[13]](#source-13).  \n",
    "\n",
    "Though both supervised and reinforcement learning use mapping between input and output, unlike supervised learning, which provides feedback to the agent in the form of the correct set of actions for performing a task, reinforcement learning uses rewards and punishment as signals for positive and negative behaviour [[13]](#source-13).  \n",
    "\n",
    "In terms of goals, reinforcement learning differs from unsupervised learning. While the aim of unsupervised learning is to discover similarities and differences between data points, the goal of reinforcement learning is to create an appropriate action model that maximizes the agent's total cumulative reward [[13]](#source-13). The [diagram](./extra/agent_environment.png) below depicts the core concept and elements of a reinforcement learning model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbb615bf-551a-49ff-b203-13a2aac9f777",
   "metadata": {},
   "source": [
    "<p align=\"center\">\n",
    "  <img src=./extra/agent_environment.png />\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d25b9d1-1358-465c-873e-a281e8732ac1",
   "metadata": {},
   "source": [
    "In recent years, space organizations and corporations have launched a considerable number of **EO satellites**. End consumers are bombarded with a massive quantity of images of diverse nature, such as optical vs. radar, high-resolution vs. wide-coverage, mono- vs. multi-spectral, often in regular time series. Fully automated analytic approaches, which require new tools to extract reliable and expressive information, are critical to their full use [[12]](#source-12). \n",
    "\n",
    "Reinforcement learning offers considerable potential for meeting the difficult demands of **remote sensing (RS) image processing**. It makes use of the massive computational capacity of current **GPUs** to do human-like reasoning and extract compact characteristics that encapsulate the semantics of input pictures. The RS community's interest in **deep reinforcement learning** approaches is **expanding** rapidly, and numerous architectures have been developed in recent years to handle RS challenges, frequently with exceptional performance [[12]](#source-12). \n",
    "\n",
    "The major areas of interest for RL applied to EO are listed below [[12]](#source-12): \n",
    "\n",
    "- Reinforcement learning for RS image understanding (e.g., object detection [[2]](#source-2) [[4]](#source-4) [[7]](#source-7) [[9]](#source-9)).\n",
    "- Deep reinforcement learning for RS image processing (e.g., classification [[5]](#source-5) [[6]](#source-6) [[8]](#source-8)).\n",
    "- Deep reinforcement learning for RS data fusion (e.g., pan-sharpening [[10]](#source-10)).\n",
    "\n",
    "In case you are curious about object detection, please take a look at the [Related Work](#related_work). Stanford University [[3]](#source-3) [[4]](#source-4) [[10]](#source-10) and Barcelona Supercomputing Center [[2]](#source-2) are significantly researching object detection in images guided by a deep reinforcement learning agent. The key idea is to focus on those parts of the image that contain richer information and zoom on them. In addition, there is an extensive research everything related to YOLO architecture as it is a very succesfull real-time object detection model.\n",
    "\n",
    "Let's take a look at this **automatic image cropping** to understand more about **reinforcement learning**. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9737c3b-540d-49dc-9e9a-3b498496cbca",
   "metadata": {},
   "source": [
    "## 2 - Architecture used for automatic image cropping <a id='image_cropping'></a>\n",
    "\n",
    "The model for **A2-RL** can be designed as an sequential decision-making process, and therefore, an agent interacts with the environment, and takes a series of actions to maximize the accumulated reward. Image cropping methods can be divided into two classes, attention-based and aesthetics-based methods. Here, the reward will be based on aesthetics [[1]](#source-1).\n",
    "\n",
    "The diagram below illustrates the difference between the two classes [[11]](#source-11)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbca5e59-d5fd-484b-89e1-f4c2128ec69b",
   "metadata": {},
   "source": [
    "![image](./extra/attention_aesthetics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39602366-785c-4c6d-85f9-1f4de7b497ef",
   "metadata": {},
   "source": [
    "For this specific problem, the agent receives observations from the input image and the cropping window. The action is then sampled from the action space based on the observation and historical experience. The sampled action is executed by the agent to change the form and position of the cropping window. After each action, the agent is rewarded based on the aesthetic score of the cropped image. By maximizing the cumulative reward, the agent attempts to identify the most appealing window in the original image [[1]](#source-1). \n",
    "\n",
    "This architecture, as previously stated, has two learning approaches. A 5-layer **CNN** and a fully-connected layer that generates a 1000-dimensional vector are used for the feature representation or cropping window. This section can be classified as weakly supervised learning. The model then divides into two branches: the actor-critic branch and the aesthetic quality evaluation branch. The actor-critic branch is made up of **three fully-connected layers** as well as a **LSTM layer**. The actor-critic branch has two outputs: the policy output, which is also known as **Actor**, and the value output, which is also known as **Critic**. The **asynchronous advantage actor-critic** algorithm is utilized for image cropping policy (A3C). The policy output is a fourteen-dimensional vector, with each dimension representing the likelihood of performing appropriate action. The value output is the current state estimation, which is the predicted cumulative reward in the present scenario. The aesthetic quality evaluation branch generates an aesthetic quality score for the cropped image, which is then utilized to calculate the reward [[1]](#source-1). \n",
    "\n",
    "In this model, both the global feature and the local feature are 1000-dim vectors, three fully-connected layers and the LSTM layer all output 1024-dim feature vectors. The detailed illustration of the **A2-RL** model architecture is shown below [[1]](#source-1):"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcc9e19-6bd8-4f3f-a638-1180b344448a",
   "metadata": {},
   "source": [
    "![image](./extra/framework.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6dfeeaa-42f0-40a3-b353-5a74dbdb3b6d",
   "metadata": {},
   "source": [
    "The two most important functions, `vfn_rl` and `auto_cropping`, are shown and explained in this notebook. The remainder of the functions are located in another [notebook](./A2RL-functions.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f5641d5-5b13-4109-9a15-4996fbf04bd5",
   "metadata": {},
   "source": [
    "### <a id='crop_input'></a> `vfn_rl`\n",
    "\n",
    "Here is where the layers of the architecture are defined. The detailed characteristics of each layer are commented along the script with `#`.\n",
    "\n",
    "First, the layers of the CNN are construed. Each layer contains a rectified linear unit activation function (ReLU). Then, the vectors inputs are normalized independently, and a max pooling on the input is performed.\n",
    "\n",
    "In the actor-critic branch, the three-fully connected layers have as well a ReLU activation function. Instead, the LSTM layer contains a sigmoid activation function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebc64e4-eaf3-4c7c-b44a-634535a78630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vfn_rl(x, variable_dict, global_feature=None, h=None, c=None, embedding_dim=1000):\n",
    "    ########################## VFN ##################################\n",
    "    ######################## Layer 1 ################################\n",
    "    ## conv1\n",
    "    #  conv(11, 11, 96, 4, 4, padding='VALID', name='conv1')\n",
    "    k_h = 11; k_w = 11; c_o = 96; s_h = 4; s_w = 4\n",
    "    conv1W = variable_dict[\"c1w\"]\n",
    "    conv1b = variable_dict[\"c1b\"]\n",
    "    conv1_in = conv(x, conv1W, conv1b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=1)\n",
    "    conv1 = tf.nn.relu(conv1_in)\n",
    "    ## lrn1\n",
    "    #  lrn(2, 2e-05, 0.75, name='norm1')\n",
    "    radius = 2; alpha = 2e-05; beta = 0.75; bias = 1.0\n",
    "    lrn1 = tf.nn.local_response_normalization(conv1,\n",
    "                                              depth_radius=radius,\n",
    "                                              alpha=alpha,\n",
    "                                              beta=beta,\n",
    "                                              bias=bias)\n",
    "    ## maxpool1\n",
    "    #  max_pool(3, 3, 2, 2, padding='VALID', name='pool1')\n",
    "    k_h = 3; k_w = 3; s_h = 2; s_w = 2; padding = 'VALID'\n",
    "    maxpool1 = tf.nn.max_pool(lrn1, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n",
    "\n",
    "    ######################## Layer 2 ################################\n",
    "    ## conv2\n",
    "    #  conv(5, 5, 256, 1, 1, group=2, name='conv2')\n",
    "    k_h = 5; k_w = 5; c_o = 256; s_h = 1; s_w = 1; group = 2\n",
    "    conv2W = variable_dict[\"c2w\"]\n",
    "    conv2b = variable_dict[\"c2b\"]\n",
    "    conv2_in = conv(maxpool1, conv2W, conv2b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "    conv2 = tf.nn.relu(conv2_in)\n",
    "    ## lrn2\n",
    "    #  lrn(2, 2e-05, 0.75, name='norm2')\n",
    "    radius = 2; alpha = 2e-05; beta = 0.75; bias = 1.0\n",
    "    lrn2 = tf.nn.local_response_normalization(conv2,\n",
    "                                              depth_radius=radius,\n",
    "                                              alpha=alpha,\n",
    "                                              beta=beta,\n",
    "                                              bias=bias)\n",
    "    ## maxpool2\n",
    "    #  max_pool(3, 3, 2, 2, padding='VALID', name='pool2')\n",
    "    k_h = 3; k_w = 3; s_h = 2; s_w = 2; padding = 'VALID'\n",
    "    maxpool2 = tf.nn.max_pool(lrn2, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n",
    "\n",
    "    ######################## Layer 3 ################################\n",
    "    ## conv3\n",
    "    #  conv(3, 3, 384, 1, 1, name='conv3')\n",
    "    k_h = 3; k_w = 3; c_o = 384; s_h = 1; s_w = 1; group = 1\n",
    "    conv3W = variable_dict[\"c3w\"]\n",
    "    conv3b = variable_dict[\"c3b\"]\n",
    "    conv3_in = conv(maxpool2, conv3W, conv3b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "    conv3 = tf.nn.relu(conv3_in)\n",
    "\n",
    "    ######################## Layer 4 ################################\n",
    "    ## conv4\n",
    "    #  conv(3, 3, 384, 1, 1, group=2, name='conv4')\n",
    "    k_h = 3; k_w = 3; c_o = 384; s_h = 1; s_w = 1; group = 2\n",
    "    conv4W = variable_dict[\"c4w\"]\n",
    "    conv4b = variable_dict[\"c4b\"]\n",
    "    conv4_in = conv(conv3, conv4W, conv4b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "    conv4 = tf.nn.relu(conv4_in)\n",
    "\n",
    "    ######################## Layer 5 ################################\n",
    "    ## conv5\n",
    "    #  conv(3, 3, 256, 1, 1, group=2, name='conv5')\n",
    "    k_h = 3; k_w = 3; c_o = 256; s_h = 1; s_w = 1; group = 2\n",
    "    conv5W = variable_dict[\"c5w\"]\n",
    "    conv5b = variable_dict[\"c5b\"]\n",
    "    conv5_in = conv(conv4, conv5W, conv5b, k_h, k_w, c_o, s_h, s_w, padding=\"SAME\", group=group)\n",
    "    conv5 = tf.nn.relu(conv5_in)\n",
    "    ## maxpool5\n",
    "    #  max_pool(3, 3, 2, 2, padding='VALID', name='pool5')\n",
    "    with tf.variable_scope(\"conv5\"):\n",
    "        k_h = 3; k_w = 3; s_h = 2; s_w = 2; padding = 'VALID'\n",
    "        maxpool5 = tf.nn.max_pool(conv5, ksize=[1, k_h, k_w, 1], strides=[1, s_h, s_w, 1], padding=padding)\n",
    "        bn5 = keras.layers.Flatten()(maxpool5)\n",
    "\n",
    "    ####################### Layer 6 ################################\n",
    "    fc6W =  variable_dict[\"fc6w\"]\n",
    "    fc6b = variable_dict[\"fc6b\"]\n",
    "    fc6 = tf.nn.relu_layer(bn5, fc6W, fc6b)\n",
    "\n",
    "    if global_feature is None:\n",
    "        return fc6\n",
    "\n",
    "    ########################### RL ##################################\n",
    "    x_rl = tf.concat([global_feature, fc6], axis=1)\n",
    "    ############## Layer 1 #################\n",
    "    fc1W = variable_dict['fc1.weight'].T\n",
    "    fc1b = variable_dict['fc1.bias']\n",
    "    fc1 = tf.nn.relu_layer(x_rl, fc1W, fc1b)\n",
    "\n",
    "    ############## Layer 2 #################\n",
    "    fc2W = variable_dict['fc2.weight'].T\n",
    "    fc2b = variable_dict['fc2.bias']\n",
    "    fc2 = tf.nn.relu_layer(fc1, fc2W, fc2b)\n",
    "\n",
    "    ############## Layer 3 #################\n",
    "    fc3W = variable_dict['fc3.weight'].T\n",
    "    fc3b = variable_dict['fc3.bias']\n",
    "    fc3 = tf.nn.relu_layer(fc2, fc3W, fc3b)\n",
    "\n",
    "    ############## LSTM #################\n",
    "    W = tf.matmul(fc3, variable_dict['lstm.weight_ih'].T) + variable_dict['lstm.bias_ih'] +\\\n",
    "        tf.matmul(h, variable_dict['lstm.weight_hh'].T) + variable_dict['lstm.bias_hh']\n",
    "    i, f, g, o = tf.split(W, 4, axis=1)\n",
    "    i = tf.sigmoid(i); f = tf.sigmoid(f); g = tf.tanh(g); o = tf.sigmoid(o);\n",
    "    c = f*c + i*g\n",
    "    h = o*tf.tanh(c)\n",
    "\n",
    "    ############## Action #################\n",
    "    action1w = variable_dict['action_fc.weight'].T\n",
    "    action1b = variable_dict['action_fc.bias']\n",
    "    action = tf.multinomial(tf.matmul(h, action1w) + action1b, 1)\n",
    "\n",
    "    return action, h, c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c337224-618e-464c-8c55-12270e8a26f9",
   "metadata": {},
   "source": [
    "### <a id='crop_input'></a> `auto_cropping`\n",
    "\n",
    "Here is where the termination action is selected. The agent will terminate the cropping operation and output the current cropping window as the final result. As the model learns when to halt the cropping process on its own, it may stop when the score no longer increases in order to obtain the optimal cropping window. The agent can theoretically cover  windows with almost arbitrary size and position on the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be5e8b06-a12e-43a8-9c11-25498ee91403",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_cropping(origin_image):\n",
    "    batch_size = len(origin_image)\n",
    "    terminals = np.zeros(batch_size)\n",
    "    ratios = np.repeat([[0, 0, 20, 20]], batch_size, axis=0)\n",
    "    img = crop_input(origin_image, generate_bbox(origin_image, ratios))\n",
    "\n",
    "    global_feature = sess.run(global_feature_placeholder, feed_dict={image_placeholder: img})\n",
    "    h_np = np.zeros([batch_size, 1024])\n",
    "    c_np = np.zeros([batch_size, 1024])\n",
    "    \n",
    "    while True:\n",
    "        action_np, h_np, c_np = sess.run((action, h, c), feed_dict={image_placeholder: img,\n",
    "                                                                    global_feature_placeholder: global_feature,\n",
    "                                                                    h_placeholder: h_np,\n",
    "                                                                    c_placeholder: c_np})\n",
    "        #print(action_np,h_np,c_np)\n",
    "        ratios, terminals = command2action(action_np, ratios, terminals)\n",
    "        #print(np.sum(terminals),batch_size,ratios)\n",
    "        bbox = generate_bbox(origin_image, ratios)\n",
    "        if np.sum(terminals) == batch_size:\n",
    "            return bbox\n",
    "        \n",
    "        img = crop_input(origin_image, bbox)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8feecc3d-7c48-4091-93c1-739cd9911d35",
   "metadata": {},
   "source": [
    "Illustration of the sequential decision-making based automatic cropping process is showing below with an image of a tree [[1]](#source-1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af675b2-f982-486a-b582-bbb6371007ac",
   "metadata": {},
   "source": [
    "![image](./extra/steps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ae920d-e61d-4192-8b74-fccc8eaa3d08",
   "metadata": {},
   "source": [
    "## 3 - Examples using Sentinel-2 images <a id='sentinel'></a>\n",
    "\n",
    "In this section, we will use two images captured from the Sentinel-2 satellite, and see if it crops and tries to detect something. It is important to remember that Sentinel-2 can be used to map changes in land cover and to monitor the world's forests. Before diving into the two EO examples, the session needs to be started and the pre-trained model has to be imported."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c402b39-a32b-47e3-8562-b1e7e0f8ab9c",
   "metadata": {},
   "source": [
    "### Loading the pre-trained model\n",
    "\n",
    "Below, the pre-trained model is imported by loading `vfn_rl.pkl`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c92a7d3-6d21-45c6-9517-1f68a744c58a",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_dtype = tf.float32\n",
    "\n",
    "with open('vfn_rl.pkl', 'rb') as f:\n",
    "    var_dict = pickle.load(f)\n",
    "\n",
    "image_placeholder = tf.placeholder(dtype=global_dtype, shape=[None,227,227,3])\n",
    "global_feature_placeholder = vfn_rl(image_placeholder, var_dict)\n",
    "\n",
    "h_placeholder = tf.placeholder(dtype=global_dtype, shape=[None,1024])\n",
    "c_placeholder = tf.placeholder(dtype=global_dtype, shape=[None,1024])\n",
    "action, h, c = vfn_rl(image_placeholder, var_dict, global_feature=global_feature_placeholder,\n",
    "                                                           h=h_placeholder, c=c_placeholder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582f0101-58dc-41b8-bbcd-7ec34ccbdbbb",
   "metadata": {},
   "source": [
    "<a id='instruction'></a>\n",
    "\n",
    "After the pre-trained model is successfully loaded, the session can be launched by using `tf.Session()`. Then, it is important to define the `image_path` and the `save_path`. The `image_path` is the location of the source image, and the `save_path` is the location where you would like to save the cropped image. It is important to note that `image_path` and `save_path` are attributes that each time need to be specified. \n",
    "\n",
    "For each iteration, the source image is loaded and it is normalized by dividing 255. This is because the RGB additive color model is utilized. Then, the new dimensions of the cropped image are defined.  The agent's actions adjust the shape and position by 0.05 times of the original image size, which could capture more accurate cropping windows than a large scale. Following that, the new image is saved in the location mentioned in `save_path`. \n",
    "\n",
    "The code in order to launch a session is illustrated below:\n",
    "\n",
    "```\n",
    "image_path = `IMAGE_PATH`\n",
    "save_path = `CROPPED_IMAGE_PATH`\n",
    "\n",
    "sess = tf.Session() \n",
    "\n",
    "args = easydict.EasyDict({\n",
    "        \"image_path\": image_path,\n",
    "        \"save_path\": save_path\n",
    "})\n",
    "\n",
    "im = io.imread(args.image_path).astype(np.float32) / 255\n",
    "xmin, ymin, xmax, ymax = auto_cropping([im - 0.5])[0]\n",
    "io.imsave(args.save_path, img_as_ubyte(im[ymin:ymax, xmin:xmax]))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e23113-e716-4ff2-95c6-71a20c976cc5",
   "metadata": {},
   "source": [
    "### Atokos, Greece - Island\n",
    "\n",
    "Here, the source image captured from Sentinel-2 on the 27$^{\\text{th}}$ of July 2018 is illustrating a Greek Island, denoted Atokos. The source image is displayed below and taken from [WEkEO](https://www.wekeo.eu/data?view=viewer&t=1532692856692&z=0&center=20.81023%2C38.48115&zoom=20.72&layers=W3siaWQiOiJjMCIsImxheWVySWQiOiJFTzpFU0E6REFUOlNFTlRJTkVMLTI6TVNJL19fREVGQVVMVF9fLzFfVFJVRV9DT0xPUiIsInpJbmRleCI6MTB9XQ%3D%3D):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fe0f2f-1103-4059-a43d-8fbbb032d613",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display(Image(filename='test_images/Atokos27-07-2018-0.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3353a9-dc8e-47a7-9509-538d70be9e39",
   "metadata": {},
   "source": [
    "Several iterations are performed in order to understand whether the agent is taking the correct decisions, and see if detects the island. As there is not a large amount of noise and the color of the water is almost the same, the agent should crop the image correctly. After a couple of iterations, all the images are collected to see the steps performed by the agent.\n",
    "\n",
    "#### **Iteration 1** <a id='island-it1'></a>\n",
    "\n",
    "Here, we follow the [instructions](#instruction) explained before. The `image_path` is the location of the source image, and the `save_path` is the location of the new cropped image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7040d5e7-9fbc-4016-a853-24fef5994128",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_path = './test_images/Atokos27-07-2018-0.png'\n",
    "save_path = './test_images/Atokos27-07-2018-1.png'\n",
    "\n",
    "sess = tf.Session() \n",
    "\n",
    "args = easydict.EasyDict({\n",
    "        \"image_path\": image_path,\n",
    "        \"save_path\": save_path\n",
    "})\n",
    "\n",
    "im = io.imread(args.image_path).astype(np.float32) / 255\n",
    "xmin, ymin, xmax, ymax = auto_cropping([im - 0.5])[0]\n",
    "io.imsave(args.save_path, img_as_ubyte(im[ymin:ymax, xmin:xmax]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75030ff-34a6-4a7a-8996-6fb2f7779f83",
   "metadata": {},
   "source": [
    "The first iteration has been executed, and one would like to see the result of the new cropped image. This is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2599959f-a98c-4a90-a023-41bc38016918",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='test_images/Atokos27-07-2018-1.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b1e397-0dc8-4749-8bf0-e506faaa0c7f",
   "metadata": {},
   "source": [
    "The agent seems that has performed the correct action, and thus another iteration will be executed.\n",
    "\n",
    "#### **Iteration 2** <a id='island-it2'></a>\n",
    "\n",
    "For this iteration, the `image_path` will be the `save_path` of [Iteration 1](#island-it1). This is because we would like to see whether the agent tries to zoom the island. A different `save_path` is defined for the new cropped image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f0a82f-9eb7-46ea-8501-ce06bc43348a",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = './test_images/Atokos27-07-2018-1.png'\n",
    "save_path = './test_images/Atokos27-07-2018-2.png'\n",
    "\n",
    "sess = tf.Session() \n",
    "\n",
    "args = easydict.EasyDict({\n",
    "        \"image_path\": image_path,\n",
    "        \"save_path\": save_path\n",
    "})\n",
    "\n",
    "im = io.imread(args.image_path).astype(np.float32) / 255\n",
    "xmin, ymin, xmax, ymax = auto_cropping([im - 0.5])[0]\n",
    "io.imsave(args.save_path, img_as_ubyte(im[ymin:ymax, xmin:xmax]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd528f10-3b7a-4807-975c-1727cc8455b8",
   "metadata": {},
   "source": [
    "The second iteration has been executed, and one would like to see the result of the new cropped image. This is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888603fa-ab19-4380-b00a-5986178ddefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='test_images/Atokos27-07-2018-2.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "570ef829-e0fb-49cb-b95c-990b0c156201",
   "metadata": {},
   "source": [
    "Again, the agent seems that has performed the correct action, and thus another iteration will be executed.\n",
    "\n",
    "#### **Iteration 3** <a id='island-it3'></a>\n",
    "\n",
    "For this iteration, the `image_path` will be the `save_path` of [Iteration 2](#island-it2). This is because we would like to see whether the agent tries to zoom the island. A different `save_path` is defined for the new cropped image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa62c413-29fc-4c24-9844-1fe335d0c097",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = './test_images/Atokos27-07-2018-2.png'\n",
    "save_path = './test_images/Atokos27-07-2018-3.png'\n",
    "\n",
    "sess = tf.Session() \n",
    "\n",
    "args = easydict.EasyDict({\n",
    "        \"image_path\": image_path,\n",
    "        \"save_path\": save_path\n",
    "})\n",
    "\n",
    "im = io.imread(args.image_path).astype(np.float32) / 255\n",
    "xmin, ymin, xmax, ymax = auto_cropping([im - 0.5])[0]\n",
    "io.imsave(args.save_path, img_as_ubyte(im[ymin:ymax, xmin:xmax]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ba826fe-58f7-437d-8357-7ceeec243d7c",
   "metadata": {},
   "source": [
    "The third iteration has been executed, and one would like to see the result of the new cropped image. This is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bde296a-941c-4b35-9191-ae01f2f618a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='test_images/Atokos27-07-2018-3.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc4be86-516c-4585-991d-e2cb6ed60cc4",
   "metadata": {},
   "source": [
    "One more time, the agent has zoomed correctly into the island. Hence, now we combine all the four images and see whether the amount of water has been decreased.\n",
    "\n",
    "#### **Combining all iterations**\n",
    "\n",
    "Here, the initial source image with the three new cropped images are illustrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa98496-eee2-4136-9d94-3a4d667eefd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_filepath = []\n",
    "for image in range(0,4):\n",
    "    images_filepath.append('test_images/Atokos27-07-2018-'+str(image)+'.png')\n",
    "    \n",
    "ipyplot.plot_images(images_filepath, max_images=20, img_width=275)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70cbcd9-5eee-45e1-befe-29a6419fa76b",
   "metadata": {},
   "source": [
    "It is important to note that the agent zooms correctly to the island. However, the sizes and dimensions are relatively the same, and this not show whether the amount of water has been reduced. Hence, it is displayed in the following approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2838bace-0570-4893-8cb7-11e6401eb493",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in range(0,4):\n",
    "    if image==0:\n",
    "        print(\"Source Image:\")\n",
    "    else:\n",
    "        print(\"Iteration \"+str(image)+\":\")\n",
    "    my_image = Image('test_images/Atokos27-07-2018-'+str(image)+'.png')\n",
    "    display(my_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b60d2de-d6f1-4bb4-b952-9c81db531b54",
   "metadata": {},
   "source": [
    "The figures above show that the dimensions of each new cropped image are becoming smaller. In addition to this, the agent is zooming into the island, and therefore the decisions taken and made are very satisfactory.\n",
    "\n",
    "## Vesuvius, Italy - Volcano\n",
    "\n",
    "Here, the source image captured from Sentinel-2 on the 2$^\\text{nd}$ of July 2019 is illustrating an Italian Volcano, named Vesuvius. The source image is displayed below and taken from [WEkEO](https://www.wekeo.eu/data?view=viewer&t=1562111376377&z=0&center=14.39623%2C40.81018&zoom=20.94&layers=W3siaWQiOiJjMCIsImxheWVySWQiOiJFTzpFU0E6REFUOlNFTlRJTkVMLTI6TVNJL19fREVGQVVMVF9fLzFfVFJVRV9DT0xPUiIsInpJbmRleCI6MTB9XQ%3D%3D&initial=1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda9a8cf-73bd-407f-b404-4eb76acd875c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='test_images/Vesuvius-02-07-2019-0.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01f9f4e6-da6b-4288-a265-778cb21ef555",
   "metadata": {},
   "source": [
    "Several iterations are performed in order to understand whether the agent is taking the correct decisions, and see if detects the crater of the volcano. This time, there is more noise than the previous example. Thus, the agent should have more difficulties in cropping the image correctly. After a couple of iterations, all the images are collected to see the steps performed by the agent.\n",
    "\n",
    "#### **Iteration 1** <a id='volcano-it1'></a>\n",
    "\n",
    "Here, we follow the [instructions](#instruction) explained before. The `image_path` is the location of the source image, and the `save_path` is the location of the new cropped image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6b646c0-62c0-4065-996c-211c6b838d42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "image_path = './test_images/Vesuvius-02-07-2019-0.png'\n",
    "save_path = './test_images/Vesuvius-02-07-2019-1.png'\n",
    "\n",
    "sess = tf.Session() \n",
    "\n",
    "args = easydict.EasyDict({\n",
    "        \"image_path\": image_path,\n",
    "        \"save_path\": save_path\n",
    "})\n",
    "\n",
    "im = io.imread(args.image_path).astype(np.float32) / 255\n",
    "xmin, ymin, xmax, ymax = auto_cropping([im - 0.5])[0]\n",
    "io.imsave(args.save_path, img_as_ubyte(im[ymin:ymax, xmin:xmax]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fccae0e5-7757-4ac3-9750-e16d391406b8",
   "metadata": {},
   "source": [
    "The first iteration has been executed, and one would like to see the result of the new cropped image. This is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44451b4-2c7f-425d-a96a-e3004811aa5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='test_images/Vesuvius-02-07-2019-1.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae426de8-d6e6-497b-be25-394b2a76e4e1",
   "metadata": {},
   "source": [
    "The agent seems that has performed the correct action, and thus another iteration will be executed.\n",
    "\n",
    "#### **Iteration 2** <a id='volcano-it2'></a>\n",
    "\n",
    "For this iteration, the `image_path` will be the `save_path` of [Iteration 1](#volcano-it1). This is because we would like to see whether the agent tries to zoom the crater of the volcano. A different `save_path` is defined for the new cropped image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b7894a-dc24-4c14-96e8-6170ca4345f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = './test_images/Vesuvius-02-07-2019-1.png'\n",
    "save_path = './test_images/Vesuvius-02-07-2019-2.png'\n",
    "\n",
    "sess = tf.Session() \n",
    "\n",
    "args = easydict.EasyDict({\n",
    "        \"image_path\": image_path,\n",
    "        \"save_path\": save_path\n",
    "})\n",
    "\n",
    "im = io.imread(args.image_path).astype(np.float32) / 255\n",
    "xmin, ymin, xmax, ymax = auto_cropping([im - 0.5])[0]\n",
    "io.imsave(args.save_path, img_as_ubyte(im[ymin:ymax, xmin:xmax]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6891c262-bbdd-4f34-b999-d7322179b33d",
   "metadata": {},
   "source": [
    "The second iteration has been executed, and one would like to see the result of the new cropped image. This is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aea65a7-55b7-4654-b27b-c0fbc3f9611f",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='test_images/Vesuvius-02-07-2019-2.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fcc3d2-3313-4c9b-9126-3cd417ac1668",
   "metadata": {},
   "source": [
    "Again, the agent seems that has performed the correct action, and thus another iteration will be executed.\n",
    "\n",
    "#### **Iteration 3** <a id='volcano-it3'></a>\n",
    "\n",
    "For this iteration, the `image_path` will be the `save_path` of [Iteration 2](#volcano-it2). This is because we would like to see whether the agent tries to zoom the crater of the volcano. A different `save_path` is defined for the new cropped image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8521bf0-00dd-457c-8e8d-90aa70cb5640",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = './test_images/Vesuvius-02-07-2019-2.png'\n",
    "save_path = './test_images/Vesuvius-02-07-2019-3.png'\n",
    "\n",
    "sess = tf.Session() \n",
    "\n",
    "args = easydict.EasyDict({\n",
    "        \"image_path\": image_path,\n",
    "        \"save_path\": save_path\n",
    "})\n",
    "\n",
    "im = io.imread(args.image_path).astype(np.float32) / 255\n",
    "xmin, ymin, xmax, ymax = auto_cropping([im - 0.5])[0]\n",
    "io.imsave(args.save_path, img_as_ubyte(im[ymin:ymax, xmin:xmax]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dfa732-9151-479a-8d40-9e714e7ca7d8",
   "metadata": {},
   "source": [
    "The third iteration has been executed, and one would like to see the result of the new cropped image. This is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0037c09d-8483-4a44-a9af-40344aef310c",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='test_images/Vesuvius-02-07-2019-3.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2150a3-8cf6-4e90-afd0-370b0776818b",
   "metadata": {},
   "source": [
    "One more time, the agent has zoomed correctly into the crater of the volcano. Hence, now we combine all the four images and see whether the amount of land around the crater has been decreased.\n",
    "\n",
    "#### **Combining all iterations**\n",
    "\n",
    "Here, the initial source image with the three new cropped images are illustrated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e28b6da-fc4e-43c4-abfb-35464f235eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_filepath = []\n",
    "for image in range(0,4):\n",
    "    images_filepath.append('test_images/Vesuvius-02-07-2019-'+str(image)+'.png')\n",
    "    \n",
    "ipyplot.plot_images(images_filepath, max_images=20, img_width=250);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3c0ab4-0450-4b67-a556-4b68ae87d675",
   "metadata": {},
   "source": [
    "It is important to note that the agent zooms correctly to the crater. However, the sizes and dimensions are relatively the same, and this not show whether the amount of land around the crater has been reduced. Hence, it is displayed in the following approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ed02ad-41a8-4f4e-ae9f-a0196869bbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for image in range(0,4):\n",
    "    if image==0:\n",
    "        print(\"Source Image:\")\n",
    "    else:\n",
    "        print(\"Iteration \"+str(image)+\":\")\n",
    "    my_image = Image('test_images/Vesuvius-02-07-2019-'+str(image)+'.png')\n",
    "    display(my_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbde766d-26ac-4650-8355-4f4621f72609",
   "metadata": {},
   "source": [
    "The figures above show that the dimensions of each new cropped image are becoming smaller. In addition to this, the agent is zooming into the crater, and therefore the decisions taken and made are very satisfactory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49df042-d77d-4d25-8b8b-b23db29b210d",
   "metadata": {},
   "source": [
    "## 4 - Conclusions <a id='conclusions'></a>\n",
    "\n",
    "- Explain the benefits of reinforcement learning in EO\n",
    "\n",
    "- Why this is useful?\n",
    "\n",
    "- Play around with adding new pictures taken from WEkEO and see if the auto-cropping is performed correctly, or increase the number of iterations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d027f72-0261-4b61-98a8-cec588a3617a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(Image(filename='test_images/Etna-02-07-2019-0.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b781d073-ec08-4e62-bb78-6474127f8c12",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "<a href=\"../00_index.ipynb\"><< Index</a><br>\n",
    "<a href=\"./\"><< Placeholder 1</a><span style=\"float:right;\"><a href=\"./\">Placeholder 2 >></a></span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e542693-3a0f-4dda-bb13-b658d4c9c8cd",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efde9a9e-0961-4f2a-9056-9dab14eb4bb8",
   "metadata": {},
   "source": [
    "<img src='../../../img/copernicus_logo.png' alt='Copernicus logo' align='left' width='20%'></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6b25fa-13f3-4e39-8c1b-c8e88750c0be",
   "metadata": {},
   "source": [
    "Course developed for [EUMETSAT](https://www.eumetsat.int/), [ECMWF](https://www.ecmwf.int/) and [Mercator Ocean International](https://www.mercator-ocean.fr/en/) in support of the [EU’s Copernicus Programme](https://www.copernicus.eu/en) and the [WEkEO platform](https://wekeo.eu/).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
